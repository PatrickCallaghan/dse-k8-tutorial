# Accessing our Service from outside the k8 cluster

From within the cluster, we can easily access our service: we just have to specify the name and k8s DNS will resolve it. We can even ask k8 to set a specific IP for our service.

Things get more complicated when we need to access our services from the outside of the k8 cluster. There are multiple way to address this problem.

## Using hostNetwork
`spec.hostNetwork: true` can be used to expose the node interface to the host.  An application that is configured to listen on all network interfaces will in turn be accessible on all network interfaces of the host machine. 

This setting can be used for DSE to expose the Cassandra node to the outside and access the C* network through the node IP itself.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: simple-app
spec:
  hostNetwork: true
  containers:
  - name: simple-app
    image: 553261234129.dkr.ecr.eu-west-2.amazonaws.com/k8s-training:quentin
    ports:
    - containerPort: 8080
```

However, this is a privileged operation in most k8s provider making this option unrealistic. 
It also doesn't solve the fact that the IP will change whenever a pod is restarted on another node, and that 2 pods can't be started on the same instance.

## Using Hostport
The hostPort feature allows to expose a single container port on the host IP. 
Using the hostPort to expose an application to the outside of the Kubernetes cluster has the same drawbacks as the hostNetwork approach discussed in the previous section. 
The host IP can change when the container is restarted, two containers using the same hostPort cannot be scheduled on the same node and the usage of the hostPort is considered a privileged operation.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: simple-app #pod name
  labels:
    app: simple-app
spec:
  containers:
  - name: simple-app #pod name
    image: 553261234129.dkr.ecr.eu-west-2.amazonaws.com/k8s-training:quentin
    ports:
    - containerPort: 8080
      hostPort: 8080
```

### using NodePort

A better way is to use a NodePort service. By default, a NodePort service will start a port on *all* the k8s nodes and redirect the traffic to our port: 
Nodeport has a valid port range of 30000-32767 by default.

vim simple-app-external.yaml :
```bash
apiVersion: v1
kind: Pod
metadata:
  name: simple-app #pod name
  labels:
    app: simple-app
spec:
  containers:
  - name: simple-app #pod name
    image: 553261234129.dkr.ecr.eu-west-2.amazonaws.com/k8s-training:quentin
    ports:
    - containerPort: 8080
      hostPort: 8080

---      

apiVersion: v1
kind: Service
metadata:
  name: simple-app-external
  labels:
    app: simple-app
spec:
  ports:
  - nodePort: 30000
    port: 30000
    protocol: TCP
    targetPort: 8080
  selector:
    app: simple-app
  type: NodePort
```

We can now access our application from any of our kubernetes node: `wget 3.8.122.56:30000`


## Using a LoadBalancer
Instead of a NodePort, we can deploy an AWS LoadBalancer to to the pod round-robin.

Under the hood, a `LoadBalancer` creates a random NodePort and links the AWS ELB to this nodePort on all the k8s node/hosts. If we add or remove a node, the ELB is updated. 

```yaml
apiVersion: v1
kind: Service
metadata:
  name: simple-app-external
  labels:
    app: simple-app
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  selector:
    app: simple-app
  type: LoadBalancer
``` 
Once it's done, we can connect to the AWS console to get the ELB IP and/or use:

```yaml
kubectl describe services 
...
LoadBalancer Ingress:     af0eee91050bd11e9b3fa068fac4e8d9-1171167347.eu-west-2.elb.amazonaws.com
...
```
NOTE: make sure you open your ELB security group to allow all inbound traffic for this test
```bash
wget af0eee91050bd11e9b3fa068fac4e8d9-1171167347.eu-west-2.elb.amazonaws.com
```

## Using an external plugin to map the loadbalancer to an external DNS
This is great, but AWS doesn't let you specify a static IP to your ELB (it's an AWS-k8 plugin restriction, seems to be the same on OpenShift, but should be ok on EKS and GKS?)
Because we can't map the LoadBalancer to a static IP, another way to access the LoadBalancer from the outside is to automatically link a DNS to the ELB IP.

On AWS, we can use a external-dns service to map the ELB against a route53 DNS entry. https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/aws.md
 
Edit your kops setup to allow the nodes to change your route53 settings:
```bash
kops edit cluster ${CLUSTER_NAME}
```
```yaml
spec:
  additionalPolicies:
    node: |
      [
        {
          "Effect": "Allow",
          "Action": [
            "route53:ChangeResourceRecordSets"
          ],
          "Resource": [
            "arn:aws:route53:::hostedzone/*"
          ]
        },
        {
          "Effect": "Allow",
          "Action": [
            "route53:ListHostedZones",
            "route53:ListResourceRecordSets"
          ],
          "Resource": [
            "*"
          ]
        }
      ]
    master: |
      [
        {
          "Effect": "Allow",
          "Action": [
            "route53:ChangeResourceRecordSets"
          ],
          "Resource": [
            "arn:aws:route53:::hostedzone/*"
          ]
        },
        {
          "Effect": "Allow",
          "Action": [
            "route53:ListHostedZones",
            "route53:ListResourceRecordSets"
          ],
          "Resource": [
            "*"
          ]
        }
      ]
```

Now you can run a cluster update to have the changes take effect:
```bash
kops update cluster ${CLUSTER_NAME} --yes
```
Kops is secured by default with RBAC (role based acces control), externalDNS will need to have access to k8 api, to make it easier we can set the default user as admin:
```bash
kubectl create clusterrolebinding default-admin --clusterrole cluster-admin --serviceaccount=default:default
```
Start by installing the route53 mapper addon:

!! MAKE SURE YOU CHANGE YOUR <<PUT_UNIQUE_ID_HERE>> !!

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: external-dns
spec:
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: external-dns
    spec:
      containers:
      - name: external-dns
        image: registry.opensource.zalan.do/teapot/external-dns:v0.5.11
        args:
        - --source=service
        - --source=ingress
        - --provider=aws
        - --registry=txt
        - --txt-owner-id=<<PUT_UNIQUE_ID_HERE>>
        - --log-level=debug
```
Create the external service deployment
```bash
kubectl apply -f external-dns.yml
```

We'll first try with a simple test using an image of our simple app on http.

The only think we need is to add `external-dns.alpha.kubernetes.io/hostname: test.k8dsetraining.com.` to our ELB Service.  Obviously, a hosted zone must exists in Route53 for the domain name we'll be using.

!! USE A UNIQUE DNS ENTRY !!
```yaml
apiVersion: v1
kind: Service
metadata:
  name: simple-app-external
  labels:
    app: simple-app
  annotations:
    external-dns.alpha.kubernetes.io/hostname: <<UNIQUE_ENTRY>>.dse-k8s-training.com.
    external-dns.alpha.kubernetes.io/ttl: "60"
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  selector:
    app: simple-app
  type: LoadBalancer
```

Make sure the dns entries are properly updated:
```bash
kubectl logs -f external-dns-6d5668477-t6jkm
time="2019-03-27T18:58:15Z" level=debug msg="Considering zone: /hostedzone/Z2A0BLYLKNVTZP (domain: dse-k8s-training.com.)"
time="2019-03-27T18:58:15Z" level=debug msg="Considering zone: /hostedzone/Z2A0BLYLKNVTZP (domain: dse-k8s-training.com.)"
time="2019-03-27T18:58:16Z" level=debug msg="Considering zone: /hostedzone/Z2A0BLYLKNVTZP (domain: dse-k8s-training.com.)"
time="2019-03-27T18:58:16Z" level=debug msg="Considering zone: /hostedzone/Z2A0BLYLKNVTZP (domain: dse-k8s-training.com.)"
time="2019-03-27T18:58:16Z" level=debug msg="Adding qa.dse-k8s-training.com. to zone dse-k8s-training.com. [Id: /hostedzone/Z2A0BLYLKNVTZP]"
time="2019-03-27T18:58:16Z" level=debug msg="Adding qa.dse-k8s-training.com. to zone dse-k8s-training.com. [Id: /hostedzone/Z2A0BLYLKNVTZP]"
time="2019-03-27T18:58:16Z" level=info msg="Desired change: CREATE qa.dse-k8s-training.com A"
time="2019-03-27T18:58:16Z" level=info msg="Desired change: CREATE qa.dse-k8s-training.com TXT"
time="2019-03-27T18:58:16Z" level=info msg="2 record(s) in zone dse-k8s-training.com. were successfully updated"
```

We can now access our pod! 

```bash
wget qa.dse-k8s-training.com
```